{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gabri\\Anaconda3\\envs\\speech2emotion\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Going to: C:\\Users\\gabri\\Documents\\Deeplearning\\Deep_emotional_recognition/data/train\n",
      "Specificly C:\\Users\\gabri\\Documents\\Deeplearning\\Deep_emotional_recognition/data/train/ang_M/\n",
      "1\n",
      "10001\n",
      "20001\n",
      "30001\n",
      "40001\n",
      "50001\n",
      "60001\n",
      "70001\n",
      "80001\n",
      "90001\n",
      "100001\n",
      "110001\n",
      "120001\n",
      "130001\n",
      "Finished with ang\n",
      "(136072, 33)\n",
      "Specificly C:\\Users\\gabri\\Documents\\Deeplearning\\Deep_emotional_recognition/data/train/dis_M/\n",
      "skipping, already done\n",
      "Specificly C:\\Users\\gabri\\Documents\\Deeplearning\\Deep_emotional_recognition/data/train/exc_M/\n",
      "skipping, already done\n",
      "Specificly C:\\Users\\gabri\\Documents\\Deeplearning\\Deep_emotional_recognition/data/train/fea_M/\n",
      "skipping, already done\n",
      "Specificly C:\\Users\\gabri\\Documents\\Deeplearning\\Deep_emotional_recognition/data/train/fru_M/\n",
      "skipping, already done\n",
      "Specificly C:\\Users\\gabri\\Documents\\Deeplearning\\Deep_emotional_recognition/data/train/hap_M/\n",
      "skipping, already done\n",
      "Specificly C:\\Users\\gabri\\Documents\\Deeplearning\\Deep_emotional_recognition/data/train/neu_M/\n",
      "skipping, already done\n",
      "Specificly C:\\Users\\gabri\\Documents\\Deeplearning\\Deep_emotional_recognition/data/train/oth_M/\n",
      "skipping, already done\n",
      "Specificly C:\\Users\\gabri\\Documents\\Deeplearning\\Deep_emotional_recognition/data/train/sad_M/\n",
      "skipping, already done\n",
      "Specificly C:\\Users\\gabri\\Documents\\Deeplearning\\Deep_emotional_recognition/data/train/sur_M/\n",
      "skipping, already done\n",
      "Specificly C:\\Users\\gabri\\Documents\\Deeplearning\\Deep_emotional_recognition/data/train/xxx_M/\n",
      "skipping, already done\n",
      "Specificly C:\\Users\\gabri\\Documents\\Deeplearning\\Deep_emotional_recognition/data/train/ang_F/\n",
      "skipping, already done\n",
      "Specificly C:\\Users\\gabri\\Documents\\Deeplearning\\Deep_emotional_recognition/data/train/dis_F/\n",
      "skipping, already done\n",
      "Specificly C:\\Users\\gabri\\Documents\\Deeplearning\\Deep_emotional_recognition/data/train/exc_F/\n",
      "skipping, already done\n",
      "Specificly C:\\Users\\gabri\\Documents\\Deeplearning\\Deep_emotional_recognition/data/train/fea_F/\n",
      "skipping, already done\n",
      "Specificly C:\\Users\\gabri\\Documents\\Deeplearning\\Deep_emotional_recognition/data/train/fru_F/\n",
      "skipping, already done\n",
      "Specificly C:\\Users\\gabri\\Documents\\Deeplearning\\Deep_emotional_recognition/data/train/hap_F/\n",
      "skipping, already done\n",
      "Specificly C:\\Users\\gabri\\Documents\\Deeplearning\\Deep_emotional_recognition/data/train/neu_F/\n",
      "skipping, already done\n",
      "Specificly C:\\Users\\gabri\\Documents\\Deeplearning\\Deep_emotional_recognition/data/train/oth_F/\n",
      "skipping, already done\n",
      "Specificly C:\\Users\\gabri\\Documents\\Deeplearning\\Deep_emotional_recognition/data/train/sad_F/\n",
      "skipping, already done\n",
      "Specificly C:\\Users\\gabri\\Documents\\Deeplearning\\Deep_emotional_recognition/data/train/sur_F/\n",
      "skipping, already done\n",
      "Specificly C:\\Users\\gabri\\Documents\\Deeplearning\\Deep_emotional_recognition/data/train/xxx_F/\n",
      "skipping, already done\n",
      "Going to: C:\\Users\\gabri\\Documents\\Deeplearning\\Deep_emotional_recognition/data/test\n",
      "Specificly C:\\Users\\gabri\\Documents\\Deeplearning\\Deep_emotional_recognition/data/test/ang_M/\n",
      "skipping, already done\n",
      "Specificly C:\\Users\\gabri\\Documents\\Deeplearning\\Deep_emotional_recognition/data/test/dis_M/\n",
      "skipping, already done\n",
      "Specificly C:\\Users\\gabri\\Documents\\Deeplearning\\Deep_emotional_recognition/data/test/exc_M/\n",
      "skipping, already done\n",
      "Specificly C:\\Users\\gabri\\Documents\\Deeplearning\\Deep_emotional_recognition/data/test/fea_M/\n",
      "skipping, already done\n",
      "Specificly C:\\Users\\gabri\\Documents\\Deeplearning\\Deep_emotional_recognition/data/test/fru_M/\n",
      "skipping, already done\n",
      "Specificly C:\\Users\\gabri\\Documents\\Deeplearning\\Deep_emotional_recognition/data/test/hap_M/\n",
      "skipping, already done\n",
      "Specificly C:\\Users\\gabri\\Documents\\Deeplearning\\Deep_emotional_recognition/data/test/neu_M/\n",
      "skipping, already done\n",
      "Specificly C:\\Users\\gabri\\Documents\\Deeplearning\\Deep_emotional_recognition/data/test/oth_M/\n",
      "skipping, already done\n",
      "Specificly C:\\Users\\gabri\\Documents\\Deeplearning\\Deep_emotional_recognition/data/test/sad_M/\n",
      "skipping, already done\n",
      "Specificly C:\\Users\\gabri\\Documents\\Deeplearning\\Deep_emotional_recognition/data/test/sur_M/\n",
      "skipping, already done\n",
      "Specificly C:\\Users\\gabri\\Documents\\Deeplearning\\Deep_emotional_recognition/data/test/xxx_M/\n",
      "skipping, already done\n",
      "Specificly C:\\Users\\gabri\\Documents\\Deeplearning\\Deep_emotional_recognition/data/test/ang_F/\n",
      "skipping, already done\n",
      "Specificly C:\\Users\\gabri\\Documents\\Deeplearning\\Deep_emotional_recognition/data/test/dis_F/\n",
      "skipping, already done\n",
      "Specificly C:\\Users\\gabri\\Documents\\Deeplearning\\Deep_emotional_recognition/data/test/exc_F/\n",
      "skipping, already done\n",
      "Specificly C:\\Users\\gabri\\Documents\\Deeplearning\\Deep_emotional_recognition/data/test/fea_F/\n",
      "skipping, already done\n",
      "Specificly C:\\Users\\gabri\\Documents\\Deeplearning\\Deep_emotional_recognition/data/test/fru_F/\n",
      "skipping, already done\n",
      "Specificly C:\\Users\\gabri\\Documents\\Deeplearning\\Deep_emotional_recognition/data/test/hap_F/\n",
      "skipping, already done\n",
      "Specificly C:\\Users\\gabri\\Documents\\Deeplearning\\Deep_emotional_recognition/data/test/neu_F/\n",
      "skipping, already done\n",
      "Specificly C:\\Users\\gabri\\Documents\\Deeplearning\\Deep_emotional_recognition/data/test/oth_F/\n",
      "skipping, already done\n",
      "Specificly C:\\Users\\gabri\\Documents\\Deeplearning\\Deep_emotional_recognition/data/test/sad_F/\n",
      "skipping, already done\n",
      "Specificly C:\\Users\\gabri\\Documents\\Deeplearning\\Deep_emotional_recognition/data/test/sur_F/\n",
      "skipping, already done\n",
      "Specificly C:\\Users\\gabri\\Documents\\Deeplearning\\Deep_emotional_recognition/data/test/xxx_F/\n",
      "skipping, already done\n",
      "Going to: C:\\Users\\gabri\\Documents\\Deeplearning\\Deep_emotional_recognition/data/validation\n",
      "Specificly C:\\Users\\gabri\\Documents\\Deeplearning\\Deep_emotional_recognition/data/validation/ang_M/\n",
      "skipping, already done\n",
      "Specificly C:\\Users\\gabri\\Documents\\Deeplearning\\Deep_emotional_recognition/data/validation/dis_M/\n",
      "skipping, already done\n",
      "Specificly C:\\Users\\gabri\\Documents\\Deeplearning\\Deep_emotional_recognition/data/validation/exc_M/\n",
      "skipping, already done\n",
      "Specificly C:\\Users\\gabri\\Documents\\Deeplearning\\Deep_emotional_recognition/data/validation/fea_M/\n",
      "skipping, already done\n",
      "Specificly C:\\Users\\gabri\\Documents\\Deeplearning\\Deep_emotional_recognition/data/validation/fru_M/\n",
      "skipping, already done\n",
      "Specificly C:\\Users\\gabri\\Documents\\Deeplearning\\Deep_emotional_recognition/data/validation/hap_M/\n",
      "skipping, already done\n",
      "Specificly C:\\Users\\gabri\\Documents\\Deeplearning\\Deep_emotional_recognition/data/validation/neu_M/\n",
      "skipping, already done\n",
      "Specificly C:\\Users\\gabri\\Documents\\Deeplearning\\Deep_emotional_recognition/data/validation/oth_M/\n",
      "skipping, already done\n",
      "Specificly C:\\Users\\gabri\\Documents\\Deeplearning\\Deep_emotional_recognition/data/validation/sad_M/\n",
      "skipping, already done\n",
      "Specificly C:\\Users\\gabri\\Documents\\Deeplearning\\Deep_emotional_recognition/data/validation/sur_M/\n",
      "skipping, already done\n",
      "Specificly C:\\Users\\gabri\\Documents\\Deeplearning\\Deep_emotional_recognition/data/validation/xxx_M/\n",
      "skipping, already done\n",
      "Specificly C:\\Users\\gabri\\Documents\\Deeplearning\\Deep_emotional_recognition/data/validation/ang_F/\n",
      "skipping, already done\n",
      "Specificly C:\\Users\\gabri\\Documents\\Deeplearning\\Deep_emotional_recognition/data/validation/dis_F/\n",
      "skipping, already done\n",
      "Specificly C:\\Users\\gabri\\Documents\\Deeplearning\\Deep_emotional_recognition/data/validation/exc_F/\n",
      "skipping, already done\n",
      "Specificly C:\\Users\\gabri\\Documents\\Deeplearning\\Deep_emotional_recognition/data/validation/fea_F/\n",
      "skipping, already done\n",
      "Specificly C:\\Users\\gabri\\Documents\\Deeplearning\\Deep_emotional_recognition/data/validation/fru_F/\n",
      "skipping, already done\n",
      "Specificly C:\\Users\\gabri\\Documents\\Deeplearning\\Deep_emotional_recognition/data/validation/hap_F/\n",
      "skipping, already done\n",
      "Specificly C:\\Users\\gabri\\Documents\\Deeplearning\\Deep_emotional_recognition/data/validation/neu_F/\n",
      "skipping, already done\n",
      "Specificly C:\\Users\\gabri\\Documents\\Deeplearning\\Deep_emotional_recognition/data/validation/oth_F/\n",
      "skipping, already done\n",
      "Specificly C:\\Users\\gabri\\Documents\\Deeplearning\\Deep_emotional_recognition/data/validation/sad_F/\n",
      "skipping, already done\n",
      "Specificly C:\\Users\\gabri\\Documents\\Deeplearning\\Deep_emotional_recognition/data/validation/sur_F/\n",
      "skipping, already done\n",
      "Specificly C:\\Users\\gabri\\Documents\\Deeplearning\\Deep_emotional_recognition/data/validation/xxx_F/\n",
      "skipping, already done\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "import os\n",
    "import pickle\n",
    "import h5py\n",
    "\n",
    "sequenceLength = 50\n",
    "emotions = ['ang','dis','exc','fea','fru','hap','neu','oth','sad','sur','xxx']\n",
    "# print('1')\n",
    "genders = ['M','F']\n",
    "dataset_seperation = ['train', 'test', 'validation']\n",
    "path =  os.getcwd() + '/data/'\n",
    "\n",
    "for subset in dataset_seperation:\n",
    "    subpath = path + subset\n",
    "    print('Going to: '+ subpath)\n",
    "    for gender in genders:\n",
    "        for emotion in emotions:\n",
    "            subsubpath = subpath + '/'+ emotion+'_'+gender + '/'\n",
    "            print('Specificly '+ subsubpath)\n",
    "            tmp = np.empty([999999, 33])\n",
    "            counter = 0\n",
    "            file_name = subset+'_'+emotion+'_'+gender\n",
    "#             h5f = h5py.File(file_name+'.h5','r')\n",
    "#             b = h5f[file_name][:]\n",
    "#             h5f.close()\n",
    "            if os.path.isfile('data/' +file_name+'.h5') == True:\n",
    "                print('skipping, already done')    \n",
    "                continue\n",
    "#             print(os.path.isfile(file_name+'.h5'))\n",
    "#             print('length is ' + b.shape)\n",
    "                    \n",
    "            for subdir, dirs, files in os.walk(subsubpath):\n",
    "                # Files now contains all csv we need to import'\n",
    "               \n",
    "                for file in files:\n",
    "                    with open(subsubpath+file, 'rt') as csvfile:\n",
    "                        spamreader = csv.reader(csvfile, delimiter=',', quotechar='|')\n",
    "                        pamreader = iter(spamreader)\n",
    "                        next(spamreader)\n",
    "                        for row in spamreader:\n",
    "                            \n",
    "                            selected_row = np.concatenate([ row[39:60] ,row[108:] ])\n",
    "                            tmp[counter,:] = selected_row\n",
    "                            counter+=1\n",
    "                            if counter % 10000 == 1:\n",
    "                                print(counter)\n",
    "#                             if tmp == []:\n",
    "#                                 tmp = selected_row\n",
    "#                             else:\n",
    "                                \n",
    "#                             print(selected_row.shape)\n",
    "#                             tmp.extend(list(map(float,selected_row)))\n",
    "#                                 tmp = np.vstack((tmp, selected_row))\n",
    "#                                 print(tmp.shape)\n",
    "                            \n",
    "            print('Finished with ' + emotion)\n",
    "            tmp = tmp[1:counter,:]\n",
    "            print(tmp.shape)\n",
    "            h5f = h5py.File('data/'+file_name + '.h5', 'w')\n",
    "            h5f.create_dataset(file_name, data=tmp)\n",
    "            h5f.close()\n",
    "            \n",
    "#             pickle.dump( tmp, open(subset+'_'+emotion+'_'+gender+'.p', \"wb\" ) )\n",
    "\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Categorical_label(label,emotion):\n",
    "# define the function blocks\n",
    "\n",
    "    hot_encoding = np.zeros(len(emotion))\n",
    "    hot_encoding[emotion.index(label)] = 1\n",
    "    return hot_encoding\n",
    "\n",
    "emotions = ['ang','exc','fru','neu']\n",
    "def getData(sets, emotions, genders, frame_number):\n",
    "    data_y = []\n",
    "    data_x = []\n",
    "    for subset in sets:\n",
    "        for emotion in emotions:\n",
    "            for gender in genders:\n",
    "                file_name = subset+'_'+emotion+'_'+gender\n",
    "                h5f = h5py.File(os.getcwd()+'/data/'+file_name+'.h5','r')\n",
    "#                 print(list(h5f.keys()))\n",
    "                tmp = h5f[file_name][:]\n",
    "                h5f.close()\n",
    "                hot_encoded_emotion = Categorical_label(emotion, emotions)\n",
    "                a, b = tmp.shape\n",
    "#                 print(hot_encoded_emotion.shape)\n",
    "                if data_x == []:\n",
    "                    data_x = tmp\n",
    "#                     print(a)\n",
    "                    data_y = np.tile(hot_encoded_emotion,[a, 1])\n",
    "                    \n",
    "                else:\n",
    "                    data_x = np.vstack((tmp, data_x))\n",
    "                    temp = np.tile(hot_encoded_emotion, [a, 1])\n",
    "                    data_y = np.vstack((data_y, temp))\n",
    "#                 print(data_x.shape)\n",
    "    samples = data_x.shape\n",
    "    newsize = math.trunc(samples[0]/frame_number)\n",
    "    x = np.reshape(data_x[0:newsize*frame_number,:], (newsize, frame_number, 33))\n",
    "    # print(y[::20,:].shape)\n",
    "    y = data_y[::frame_number,:]\n",
    "    y = y[0:newsize,:]\n",
    "    return x, y\n",
    "                \n",
    "    \n",
    "# x, y = getData(['train'], emotions, 'M')\n",
    "# print(x.shape)\n",
    "# print(x[800000:800000+5,:])\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 1.0, 1: 2.6719275372263778}\n",
      "x\n",
      "(13426, 20, 33)\n",
      "y\n",
      "(13426, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gabri\\Anaconda3\\envs\\speech2emotion\\lib\\site-packages\\ipykernel_launcher.py:23: DeprecationWarning: elementwise == comparison failed; this will raise an error in the future.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected dense_1_input to have shape (660,) but got array with shape (33,)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-64-7fd780e4511a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[1;31m# print(data_x.shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[1;31m# model.fit_generator(train_generator, steps_per_epoch=train_size, epochs=300,shuffle=True,class_weight=class_weight_dict)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m \u001b[1;31m# pred = model.predict_generator( train_generator_overfit, steps=test_size)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\speech2emotion\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1000\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1001\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1002\u001b[1;33m                               validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1003\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1004\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\speech2emotion\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1628\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1629\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1630\u001b[1;33m             batch_size=batch_size)\n\u001b[0m\u001b[0;32m   1631\u001b[0m         \u001b[1;31m# Prepare validation data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1632\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\speech2emotion\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[0;32m   1474\u001b[0m                                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_feed_input_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1475\u001b[0m                                     \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1476\u001b[1;33m                                     exception_prefix='input')\n\u001b[0m\u001b[0;32m   1477\u001b[0m         y = _standardize_input_data(y, self._feed_output_names,\n\u001b[0;32m   1478\u001b[0m                                     \u001b[0moutput_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\speech2emotion\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    121\u001b[0m                             \u001b[1;34m': expected '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' to have shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' but got array with shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 123\u001b[1;33m                             str(data_shape))\n\u001b[0m\u001b[0;32m    124\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking input: expected dense_1_input to have shape (660,) but got array with shape (33,)"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD\n",
    "from utils import dataset_generator\n",
    "from utils import total_number\n",
    "from utils import weight_class\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import math\n",
    "\n",
    "\n",
    "numpy.set_printoptions(threshold=numpy.inf)\n",
    "\n",
    "#41 -- 51\n",
    "\n",
    "trainable = 'True'\n",
    "#6\n",
    "numpy.random.seed(6)\n",
    "\n",
    "#emotions = ['ang','dis','exc','fea','fru','hap','neu','oth','sad','sur','xxx']\n",
    "emotions = ['sad','hap']#,'ang','exc']\n",
    "size_batch = 32\n",
    "frame_number = 20\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(254, activation='relu', input_dim=frame_number*33, name='dense_1',kernel_initializer='VarianceScaling'))\n",
    "#model.add(Dropout(0.5))\n",
    "model.add(Dense(254, activation='relu', name='dense_2',kernel_initializer='VarianceScaling'))\n",
    "#model.add(Dropout(0.5))\n",
    "model.add(Dense(254, activation='relu', name='dense_3',kernel_initializer='VarianceScaling'))\n",
    "#model.add(Dropout(0.5))\n",
    "model.add(Dense(254, activation='relu', name='dense_4',kernel_initializer='VarianceScaling'))\n",
    "#model.add(Dropout(0.5))\n",
    "#model.add(Dense(100, activation='sigmoid', trainable=trainable,name='dense_5'))\n",
    "#model.add(Dense(64, activation='sigmoid', trainable=trainable,name='dense_6'))\n",
    "#model.add(Dense(32, activation='sigmoid', trainable=trainable,name='dense_7'))\n",
    "#model.add(Dense(16, activation='sigmoid', trainable=trainable,name='dense_8'))\n",
    "#model.add(Dense(8, activation='sigmoid', trainable=trainable,name='dense_9'))\n",
    "#model.add(Dropout(0.5))\n",
    "model.add(Dense(len(emotions), activation='softmax',name='dense_55j'))\n",
    "\n",
    "\n",
    "#some possible optimizer\n",
    "\n",
    "adam =keras.optimizers.Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "sgd = SGD(lr=0.000000001, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "\n",
    "#lr=0.0000001\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=adam,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# train_size,_,_ = total_number('train','M',emotions,size_batch,frame_number)\n",
    "# validation_size,_,_ = total_number('validation','M',emotions,size_batch,frame_number)\n",
    "# test_size,_,_ = total_number('test','M',emotions,size_batch,frame_number)\n",
    "\n",
    "# print(\"\\nsize of train \"+str(train_size)+\"\\n\")\n",
    "# print(\"size of test_size \"+str(test_size)+\"\\n\")\n",
    "\n",
    "\n",
    "# train_generator = dataset_generator(size_batch,'train','M',emotions,frame_number)\n",
    "# validation_generator = dataset_generator(size_batch,'validation','M',emotions,frame_number)\n",
    "# test_generator = dataset_generator(size_batch,'test','M',emotions,frame_number)\n",
    "# train_generator_overfit = dataset_generator(size_batch,'test','M',emotions,frame_number)\n",
    "\n",
    "class_weight_dict = weight_class('train',emotions,'M')\n",
    "\n",
    "print(class_weight_dict)\n",
    "x_data, y_data = getData(['train'], emotions, 'M', 20)\n",
    "print('x')\n",
    "print(x_data.shape)\n",
    "print('y')\n",
    "print(y_data.shape)\n",
    "\n",
    "\n",
    "\n",
    "model.fit(x, y, epochs = 5, batch_size = 32)\n",
    "# pred = model.predict_generator( train_generator_overfit, steps=test_size)\n",
    "\n",
    "print(numpy.sum(pred > 1/len(emotions),axis=0))\n",
    "\n",
    "\n",
    "print(model.evaluate_generator( train_generator_overfit, steps=test_size))\n",
    "\n",
    "\n",
    "#model.load_weights('weights',by_name=False)\n",
    "#model.save_weights(\"weights\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
